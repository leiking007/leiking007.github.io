---
title: "æ¨¡å‹ç®€æ˜“éƒ¨ç½²"
description: ""
date: 2025-03-23
lastmod: 2025-03-23
tags: ["ai"]
categories: ["ğŸ§æœåŠ¡å™¨è¿ç»´"]
author: "lei"
draft: false
---

# æ¨¡å‹ç®€æ˜“éƒ¨ç½²

## vllméƒ¨ç½²æ¨¡å‹

> vllmå¯ä»¥å…¼å®¹éƒ¨ç½²åŸºäº Transformer æ¶æ„çš„æ¨¡å‹

### ç¯å¢ƒå‡†å¤‡

æœ¬åœ°é…ç½®ï¼šUbuntu24.04ã€Python12ã€RTX3070

1. å®‰è£…ç›¸å…³è½¯ä»¶åŒ…

   ```bash
   apt install python3.12-venv
   apt install git 
   apt install nvidia-cuda-toolkit
   ```

2. æ–°å»ºpythonè™šæ‹Ÿç¯å¢ƒ

   ```bash
   # æ–°å»ºvllmè™šæ‹Ÿç¯å¢ƒ
   python3 -m venv vllm
   ```

3. å®‰è£…ç›¸å…³ä¾èµ–

   ```bash
   #å…ˆæ¿€æ´»ç¯å¢ƒ
   source vllm/bin/activate
   
   # å®‰è£…modelscope ï¼ˆä¼šè‡ªåŠ¨å®‰è£… torch ã€vllm ç­‰ï¼‰
   pip install modelscope
   
   # é€€å‡ºå½“å‰æ¿€æ´»ç¯å¢ƒ
   deactivate
   ```

### ä¸‹è½½æ¨¡å‹

1. ä½¿ç”¨ modelscope è¿›è¡Œä¸‹è½½éœ€è¦çš„æ¨¡å‹æ–‡ä»¶

   æ–°å»ºä¸‹è½½è„šæœ¬ `download_model.py`

   ```python
   # -*- coding: utf-8 -*-
   """
   ä½œè€…ï¼štanglei
   æ—¥æœŸï¼š2025-03-23
   ç‰ˆæœ¬ï¼š1.0
   
   æè¿°ï¼š
   è¿™æ˜¯ä¸€ä¸ªä»modelscopeä¸‹è½½æ¨¡å‹çš„ Python è„šæœ¬
   
   ä½¿ç”¨æ–¹æ³•ï¼š
   python download_model.py --model_id okwinds/DeepSeek-V3-GGUF-V3 --revision master --cache_dir ./models
   """
   
   from modelscope.hub.snapshot_download import snapshot_download
   import argparse
   
   if __name__ == "__main__":
       parser = argparse.ArgumentParser()
       parser.add_argument(
           "--model_id", type=str, default=None, help="æ¨¡å‹ID"
       )
       parser.add_argument("--revision", type=str, default="master", help="åˆ†æ”¯å")
       parser.add_argument("--cache_dir", type=str, default="models", help="ç¼“å­˜ç›®å½•")
   
   
       # è§£æå‘½ä»¤è¡Œå‚æ•°
       args = parser.parse_args()
   
       if args.model_id is None:
           print("è¯·æä¾›æ¨¡å‹ID")
           exit(1)
   
       print(f"å¼€å§‹ä¸‹è½½æ¨¡å‹ï¼š{args.model_id}")
       print(f"ç¼“å­˜ç›®å½•ï¼š{args.cache_dir}")
   
   
       # ä¸‹è½½æŒ‡å®šæ¨¡å‹ åˆ°æœ¬åœ°æŒ‡å®šç›®å½• revision ä¸ºåˆ†æ”¯å, é»˜è®¤ä¸ºmaster, cache_dir ä¸ºç¼“å­˜ç›®å½•, é»˜è®¤ä¸º./cache
       model_dir = snapshot_download(
           model_id=args.model_id, revision=args.revision, cache_dir=args.cache_dir
       )
   
       print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°ï¼š{model_dir}")
   ```

2. æ¿€æ´» vllm è™šæ‹Ÿç¯å¢ƒ

   ```bash
   source vllm/bin/activate
   ```

3. ä½¿ç”¨è„šæœ¬ä¸‹è½½éœ€è¦çš„æ¨¡å‹ï¼Œmodel_id åœ¨ modelscopeå®˜ç½‘å¯ä»¥æ‰¾åˆ°

   ```bash
   python3 download_model.py --model_id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --cache_dir /data/model
   ```
   


### è¿è¡Œæ¨¡å‹


1. æ–°å»ºæ¨¡å‹å¯åŠ¨è„šæœ¬ `start_dp1_5b.sh`

   ```bash
   # export CUDA_VISIBLE_DEVICES=0 é™åˆ¶ç¨‹åºåªä½¿ç”¨ç¼–å·ä¸º 0 çš„ GPU è®¾å¤‡
   # --served-model-name DeepSeek-R1-Distill-Qwen-1.5B å‚æ•°ç”¨äºæŒ‡å®šåœ¨ API æœåŠ¡ä¸­å¯¹å¤–æš´éœ²çš„æ¨¡å‹åç§°
   # --model æŒ‡å®šåŠ è½½çš„æ¨¡å‹è·¯å¾„
   # --gpu-memory-utilization 0.7 é™åˆ¶ GPU æ˜¾å­˜çš„ä½¿ç”¨ç‡
   # --tensor-parallel-size 1 æŒ‡å®šå¼ é‡å¹¶è¡Œçš„å¤§å°,å¯¹äºå•å¡è¿è¡Œï¼Œé€šå¸¸è®¾ç½®ä¸º 1ã€‚å¦‚æœä½ æœ‰å¤šä¸ª GPUï¼Œå¯ä»¥è®¾ç½®ä¸ºæ›´å¤§çš„å€¼ï¼ˆå¦‚ 2 æˆ– 4ï¼‰ï¼Œä»¥å®ç°>å¼ é‡å¹¶è¡Œ
   # --max-num-seqs 32 è®¾ç½®æœ€å¤§å¹¶å‘åºåˆ—æ•°,32 è¡¨ç¤ºæœåŠ¡å™¨æœ€å¤šå¯ä»¥åŒæ—¶å¤„ç†32ä¸ªåºåˆ—ï¼ˆå¦‚æ–‡æœ¬åºåˆ—ï¼‰ã€‚è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶å¹¶å‘é‡ï¼Œé¿å…æœåŠ¡å™¨è¿‡è½½
   # --max-model-len 8192 è®¾ç½®æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦,8192 è¡¨ç¤ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ä¸º 8192 ä¸ª tokenã€‚å¦‚æœè¾“å…¥çš„åºåˆ—é•¿åº¦è¶…è¿‡è¿™ä¸ªå€¼ï¼Œå¯èƒ½ä¼šè¢«æˆªæ–­æˆ–æ‹’ç»
   # --dtype float16 æŒ‡å®šæ¨¡å‹ä½¿ç”¨çš„æ•°æ®ç±»å‹,float16 è¡¨ç¤ºä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆ16 ä½æµ®ç‚¹æ•°ï¼‰è¿›è¡Œè®¡ç®—ã€‚è¿™ç§æ•°æ®ç±»å‹å¯ä»¥å‡å°‘å†…å­˜å ç”¨å¹¶åŠ é€Ÿè®¡ç®—ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€å®šçš„ç²¾åº¦
   # --disable-log-requests ç¦ç”¨è¯·æ±‚æ—¥å¿—,æœåŠ¡å™¨ä¸ä¼šè®°å½•å®¢æˆ·ç«¯å‘é€çš„è¯·æ±‚å†…å®¹ã€‚è¿™å¯ä»¥å‡å°‘æ—¥å¿—æ–‡ä»¶çš„å¤§å°ï¼Œä½†ä¹Ÿä¼šä¸¢å¤±è¯·æ±‚çš„ è¯¦ç»†ä¿¡æ¯ã€‚
   # --disable-log-stats ç¦ç”¨ç»Ÿè®¡æ—¥å¿—,æœåŠ¡å™¨ä¸ä¼šè®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚è¯·æ±‚å¤„ç†æ—¶é—´ã€ååé‡ç­‰ï¼‰ã€‚è¿™åŒæ ·å¯ä»¥å‡å°‘æ—¥å¿—æ–‡ä»¶ çš„å¤§å°ï¼Œä½†ä¸åˆ©äºæ€§èƒ½ç›‘æ§ã€‚
   # --uvicorn-log-level warning è®¾ç½® Uvicornï¼ˆä¸€ä¸ª ASGI æœåŠ¡å™¨ï¼‰çš„æ—¥å¿—çº§åˆ«,warning è¡¨ç¤ºåªè®°å½•è­¦å‘ŠåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—ä¿¡æ¯ï¼ˆå¦‚é”™ è¯¯ã€ä¸¥é‡é”™è¯¯ç­‰ï¼‰ã€‚è¿™å¯ä»¥å¸®åŠ©å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œä¸“æ³¨äºé‡è¦é—®é¢˜ã€‚
   nohup sh -c 'CUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server \
           --api-key lei@123 \
           --served-model-name YHGPT-14B-v2.0-beta \
           --model /data/wenyu_14b_v2.0_beta_fp16 \
           --tensor-parallel-size 2 \
           --gpu-memory-utilization 0.9 \
           --host 0.0.0.0 \
           --port 8002 \
           --max-num-seqs 32 \
           --max-model-len 8192 \
           --dtype float16 \
           --disable-log-requests \
           --disable-log-stats \
           --uvicorn-log-level warning' > logs/YHGPT-14B-v2.0-beta.log 2>&1 &
   ```
   
2. å¯åŠ¨è„šæœ¬ï¼Œå¹¶æŸ¥çœ‹æ—¥å¿—

   ```bash
   # æ¿€æ´» vllm è™šæ‹Ÿç¯å¢ƒ
   source vllm/bin/activate
   
   # å¯åŠ¨è„šæœ¬
   ./start_dp7b.sh
   
   # æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
   tail -1000f logs/ds1_5b.log
   ```
   
   è¾“å‡ºå¦‚ä¸‹å†…å®¹å¯åŠ¨æˆåŠŸ
   
   ![image-20250323141949671](./assets/image-20250323141949671.png)

### æµ‹è¯•æ¨¡å‹

1. åœ¨çº¿æ–‡æ¡£: GET http://localhost:8000/redoc

2. èŠå¤©è¡¥å…¨api: POST http://localhost/v1/chat/completionsï¼Œbodyå‚æ•°

   ```json
   {
     "messages": [
       {
         "role": "system",
         "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"
       },
       {
         "role": "user",
         "content": "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚"
       }
     ],
     "model": "DeepSeek-R1-Distill-Qwen-1.5B",
     "max_tokens": 1000,
     "temperature": 0.7,
     "frequency_penalty": 0,
     "presence_penalty": 0,
     "logit_bias": {},
     "logprobs": null,
     "top_logprobs": null,
     "n": 1,
     "response_format": null,
     "seed": null,
     "stop": null,
     "stream": false,
     "stream_options": null,
     "tools": null,
     "tool_choice": "none",
     "parallel_tool_calls": false,
     "user": null
   }
   ```

## modelscope

1. ä»modelscopeå®˜ç½‘æ–‡æ¡£ï¼Œä¸‹è½½æœ€æ–°çš„modelscopeé•œåƒåŒ…

2. å¯åŠ¨å¹¶è¿›å…¥å®¹å™¨ï¼Œæ˜ å°„äº†å®¹å™¨8000ç«¯å£å’Œ/dataç›®å½•ï¼›$VOLUME_DIRæ˜¯ç¯å¢ƒå˜é‡ï¼Œä¸ºå®¹å™¨å·è·¯å¾„

   ```bash
   docker run --name modescope -it --rm -p 8000:8000 -v $VOLUME_DIR/modescope/data:/data 08030f417e0d bash
   ```

3. æ–°å»º download_model.py

   ```python
   # -*- coding: utf-8 -*-
   """
   ä½œè€…ï¼štanglei
   æ—¥æœŸï¼š2025-03-23
   ç‰ˆæœ¬ï¼š1.0
   
   æè¿°ï¼š
   è¿™æ˜¯ä¸€ä¸ªä»modelscopeä¸‹è½½æ¨¡å‹çš„ Python è„šæœ¬
   
   ä½¿ç”¨æ–¹æ³•ï¼š
   python download_model.py --model_id okwinds/DeepSeek-V3-GGUF-V3 --revision master --cache_dir ./models
   """
   
   from modelscope.hub.snapshot_download import snapshot_download
   import argparse
   
   if __name__ == "__main__":
       parser = argparse.ArgumentParser()
       parser.add_argument(
           "--model_id", type=str, default=None, help="æ¨¡å‹ID"
       )
       parser.add_argument("--revision", type=str, default="master", help="åˆ†æ”¯å")
       parser.add_argument("--cache_dir", type=str, default="models", help="ç¼“å­˜ç›®å½•")
   
   
       # è§£æå‘½ä»¤è¡Œå‚æ•°
       args = parser.parse_args()
   
       if args.model_id is None:
           print("è¯·æä¾›æ¨¡å‹ID")
           exit(1)
   
       print(f"å¼€å§‹ä¸‹è½½æ¨¡å‹ï¼š{args.model_id}")
       print(f"ç¼“å­˜ç›®å½•ï¼š{args.cache_dir}")
   
   
       # ä¸‹è½½æŒ‡å®šæ¨¡å‹ åˆ°æœ¬åœ°æŒ‡å®šç›®å½• revision ä¸ºåˆ†æ”¯å, é»˜è®¤ä¸ºmaster, cache_dir ä¸ºç¼“å­˜ç›®å½•, é»˜è®¤ä¸º./cache
       model_dir = snapshot_download(
           model_id=args.model_id, revision=args.revision, cache_dir=args.cache_dir
       )
   
       print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°ï¼š{model_dir}")
   	
   ```

4. å®‰è£…fastapi

   ```bash
   pip install fastapi
   pip install "uvicorn[standard]"
   ```



### è¯­éŸ³è¯†åˆ«æ¨¡å‹

> ç¦»çº¿éƒ¨ç½² SenseVoiceSmall æ¨¡å‹

1. ä¸‹è½½æ‰€éœ€çš„æ¨¡å‹æ–‡ä»¶

   ```bash
   python download_model.py --model_id iic/SenseVoiceSmall
   python download_model.py --model_id iic/speech_fsmn_vad_zh-cn-16k-common-pytorch
   ```

2. åˆ›å»ºæ¨¡å‹å¯åŠ¨ç¨‹åº start_iic.py

   ```python
   from fastapi import FastAPI, UploadFile, File
   import os
   import logging
   import sys
   import traceback
   from funasr import AutoModel
   from funasr.utils.postprocess_utils import rich_transcription_postprocess
   import threading
   
   # é…ç½®æ—¥å¿—
   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   
   app = FastAPI()
   
   # åˆå§‹åŒ–æ¨¡å‹
   model_dir = "models/iic/SenseVoiceSmall"
   model = AutoModel(
       model=model_dir,
       trust_remote_code=False,
       vad_model="models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
       vad_kwargs={"max_single_segment_time": 30000},
       ban_emo_unk=True,
       disable_update=True,
   )
   
   @app.post("/recognize/")
   async def recognize_audio(file: UploadFile = File(...)):
       # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
       audio_path = f"temp_audio_{threading.current_thread().ident}.wav"
       try:
           with open(audio_path, "wb") as f:
               f.write(await file.read())  # æ³¨æ„ï¼šfile.read() æ˜¯å¼‚æ­¥æ“ä½œï¼Œéœ€è¦åŠ  await
       except Exception as e:
           return {"error": f"Failed to save audio file: {str(e)}"}
       try:
           res = model.generate(
               input=audio_path,
               cache={},
               language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
               use_itn=True,
               batch_size_s=60,
               merge_vad=True,  #
               merge_length_s=15,
           )
           text = rich_transcription_postprocess(res[0]["text"])
   
           # è¿”å›è¯†åˆ«ç»“æœ
           return {"result": text}
       except Exception as e:
           # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
           exc_type, exc_value, exc_traceback = sys.exc_info()
           logging.error(f"exception type: {exc_type}; exception value: {exc_value}")
           # æ‰“å°å †æ ˆä¿¡æ¯
           traceback.print_tb(exc_traceback)
           return {"error": f"Error during recognition: {str(e)}"}
       finally:
           # åˆ é™¤ä¸´æ—¶ä¿å­˜çš„éŸ³é¢‘æ–‡ä»¶
           if os.path.exists(audio_path):
               try:
                   os.remove(audio_path)
               except PermissionError:
                   return {"error": "Permission denied when deleting the temporary audio file"}
               except Exception as e:
                   return {"error": f"Error deleting temporary audio file: {str(e)}"}
   
   
   @app.get("/")
   def read_root():
       return {"Hello": "World"}
   
   if __name__ == "__main__":
       import uvicorn
       uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info", timeout_keep_alive=60)
   ```

3. å¯åŠ¨ç¨‹åº

   ```bash
   python start_iic.py
   ```

4. æµ‹è¯•

   è®¿é—®æœåŠ¡æ˜¯å¦æ­£å¸¸

   ![image-20250506173428598](./assets/image-20250506173428598.png)

   æµ‹è¯•è¯­éŸ³è¯†åˆ«æ¨¡å‹æ˜¯å¦æ­£å¸¸

   ![image-20250506173515831](./assets/image-20250506173515831.png)

